{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Prueba 1 - Vectores - GPU.ipynb","provenance":[{"file_id":"/v2/external/notebooks/basic_features_overview.ipynb","timestamp":1601321037647}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Zw-Vno_15t-E"},"source":["# 1 Introducción\n","El siguiente cuaderno realiza la suma de dos vectores, utilizando GPGPU. El algoritmo está basado en la función axpy nivel 1[3], de la biblioteca BLAS[4] que resuelve la ecuación:\n","<center>$Y=\\alpha X + Y$</center>\n","\n","Su objetivo es enseñar a los alumnos como se utiliza Python [2] la plataforma Colab[1] y CUDA[5,6]. Mostrando el funcionamiento y granularidad (grilla, bloque, warps) de sobre una dimensión (x)."]},{"cell_type":"markdown","metadata":{"id":"7cRnhv_7N4Pa"},"source":["---\n","# 2 Armado del ambiente\n","Instala en el cuaderno el módulo CUDA de Python."]},{"cell_type":"code","metadata":{"id":"z74FNbCszDmw"},"source":["!pip install pycuda"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NzQaWRTtc1Zj"},"source":["---\n","# 3 Desarrollo\n","Ejecuta el Código CPU - GPU."]},{"cell_type":"code","metadata":{"id":"9c7mZSnu0M3m"},"source":["# --------------------------------------------\n","#@title 3.1 Parámetros de ejecución { vertical-output: true }\n","\n","cantidad_N =   50000#@param {type: \"number\"}\n","alfa =   1#@param {type: \"number\"}\n","# --------------------------------------------\n","\n","from datetime import datetime\n","\n","tiempo_total = datetime.now()\n","\n","import pycuda.driver as cuda\n","import pycuda.autoinit\n","from pycuda.compiler import SourceModule\n","\n","import numpy\n","\n","# --------------------------------------------\n","# Definición de función que transforma el tiempo en  milisegundos \n","tiempo_en_ms = lambda dt:(dt.days * 24 * 60 * 60 + dt.seconds) * 1000 + dt.microseconds / 1000.0\n","\n","\n","# CPU - Defino la memoria de los vectores en cpu.\n","x_cpu = numpy.random.randn( cantidad_N )\n","x_cpu = x_cpu.astype( numpy.float32() )\n","\n","y_cpu = numpy.random.randn( cantidad_N )\n","y_cpu = y_cpu.astype( numpy.float32() )\n","\n","#tiempo_ini_cpu = datetime.now()\n","\n","r_cpu = numpy.empty_like( x_cpu )\n","\n","# CPU - reservo la memoria GPU.\n","x_gpu = cuda.mem_alloc( x_cpu.nbytes )\n","y_gpu = cuda.mem_alloc( y_cpu.nbytes )\n","\n","# GPU - Copio la memoria al GPU.\n","cuda.memcpy_htod( x_gpu, x_cpu )\n","cuda.memcpy_htod( y_gpu, y_cpu )\n","\n","# CPU - Defino la función kernel que ejecutará en GPU.\n","module = SourceModule(\"\"\"\n","__global__ void kernel_axpy( int n, float alfa, float *X, float *Y )\n","{\n","  int idx = threadIdx.x + blockIdx.x*blockDim.x;\n","  if( idx < n )\n","  {\n","    Y[idx]  = alfa*X[idx] + Y[idx];\n","  }\n","}\n","\"\"\") \n","# CPU - Genero la función kernel.\n","kernel = module.get_function(\"kernel_axpy\")\n","\n","tiempo_gpu = datetime.now()\n","\n","# GPU - Ejecuta el kernel.\n","# TODO: Falta consultar limites del GPU, para armar las dimensiones correctamente.\n","dim_hilo = 256\n","dim_bloque = numpy.int( (cantidad_N+dim_hilo-1) / dim_hilo )\n","print( \"Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n","\n","#TODO: Ojo, con los tipos de las variables en el kernel.\n","kernel( numpy.int32(cantidad_N),numpy.float32(alfa), x_gpu, y_gpu, block=( dim_hilo, 1, 1 ),grid=(dim_bloque, 1,1) )\n","\n","tiempo_gpu = datetime.now() - tiempo_gpu\n","\n","# GPU - Copio el resultado desde la memoria GPU.\n","cuda.memcpy_dtoh( r_cpu, y_gpu )\n","\n","\"\"\"\n","# CPU - Informo el resutlado.\n","print( \"------------------------------------\")\n","print( \"X: \" )\n","print( x_cpu )\n","print( \"------------------------------------\")\n","print( \"Y: \" )\n","print( y_cpu )\n","print( \"------------------------------------\")\n","print( \"R: \" )\n","print( r_cpu )\n","\"\"\"\n","\n","tiempo_total = datetime.now() - tiempo_total\n","\n","print( \"Cantidad de elementos: \", cantidad_N )\n","print( \"Thread x: \", dim_hilo, \", Bloque x:\", dim_bloque )\n","print(\"Tiempo CPU: \", tiempo_en_ms( tiempo_total ), \"[ms]\" )\n","print(\"Tiempo GPU: \", tiempo_en_ms( tiempo_gpu   ), \"[ms]\" )\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EALIlyyG6iqP"},"source":["---\n","# 4 Tabla de pasos de ejecución del programa\n","\n","\n"," Procesador | Funciòn | Detalle\n","------------|---------|----------\n","CPU      |  @param                | Lectura del tamaño de vectores desde Colab.\n","CPU      |  import                | Importa los módulos para funcionar.\n","CPU      |  datetime.now()        | Toma el tiempo actual.\n","CPU      |  numpy.random.randn( Cantidad_N ) | Inicializa los vectoes A, B y R.\n","**GPU**  |  cuda.mem_alloc()      | Reserva la memoria en GPU.\n","**GPU**  |  cuda.memcpy_htod()    | Copia las memorias desde el CPU al GPU.\n","CPU      |  SourceModule()        | Define el código del kernel \n","CPU      |  module.get_function() | Genera la función del kernel GPU\n","CPU      |  dim_tx/dim_bx         | Calcula las dimensiones.\n","**GPU**  |  kernel()              | Ejecuta el kernel en GPU\n","CPU      |  cuda.memcpy_dtoh( )   | Copia el resultado desde GPU memoria A a CPU memoria R.\n","CPU      |  print()               | Informo los resultados.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"TzgZkrQD-UTy"},"source":["---\n","# 5 Conclusiones\n","\n","Las conclusiones son explicadas en clase....\n"]},{"cell_type":"markdown","metadata":{"id":"6hn6HOCYEjyY"},"source":["---\n","# 6 Bibliografia\n","\n","[1] MARKDOWN SYNTAX Colab: [PDF](https://github.com/wvaliente/SOA_HPC/blob/main/Documentos/markdown-cheatsheet-online.pdf)\n","\n","[2] Introducción a Python: [Página Colab](https://github.com/wvaliente/SOA_HPC/blob/main/Documentos/Python_Basico.ipynb) \n","\n","[3] Función Axpy de biblioteca BLAS: [Referencia](https://software.intel.com/content/www/us/en/develop/documentation/mkl-developer-reference-c/top/blas-and-sparse-blas-routines/blas-routines/blas-level-1-routines-and-functions/cblas-axpy.html)\n","\n","[4] Biblioteca BLAS: [Referencia](http://www.netlib.org/blas/)\n","\n","[5] Documentación PyCUDA: [WEB](https://documen.tician.de/pycuda/index.html)\n","\n","[6] Repositorio de PyCUDA: [WEB](https://pypi.python.org/pypi/pycuda)\n","\n","\n"]}]}
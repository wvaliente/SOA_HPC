{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Cuaderno 2020 mpi4py.ipynb","provenance":[{"file_id":"https://github.com/pablobermudez/Up/blob/main/HPC/Ejercicio3.ipynb","timestamp":1606589806800}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"sMZqDYYBzCLe"},"source":["# 1 Introducción\n","\n","Para este ejercicio se ha optado por aplicar el tema teórico **MPI** (message passing interface).\n","\n","La finalidad de este ejercicio será ampliar el conocimiento sobras la manera que posee Python para implementar la comunicación entre distintos procesos con el uso de una librería denominada **MPI4py**.\n","\n","Para esto, se optará por un desarrollo que utilizará un patrón Master-Slave en donde un proceso se comunique y envíe órdenes a los demás.\n","\n","En primera instancia, se debe ejecutar el armado del ambiente para instalar la libreria, luego se debe ejecutar el codigo el cual generara un codigo Python en un archivo, por ultimo se debe ejecutar el bloque de codigo en la subseccion 3.1 para poder ver los resultados."]},{"cell_type":"markdown","metadata":{"id":"r9TdOhH3zGsx"},"source":["#2 Armado del ambiente\n","\n","Instalar en el cuaderno el módulo MPI4py de Python."]},{"cell_type":"code","metadata":{"id":"DV_cRj3n4SF3"},"source":["! pip install mpi4py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aLT-Sj9UzI8E"},"source":["#3 Desarrollo"]},{"cell_type":"code","metadata":{"id":"tGvDuYLk4zpb"},"source":["%%writefile Ejercicio3.py\n","from mpi4py import MPI\n","import numpy as np\n","import time\n","\n","# --------------------------------------------\n","# Formulario\n","Max_tiempo_sleep =   5#@param {type: \"number\"}\n","Min_tiempo_sleep =   1#@param {type: \"number\"}\n","# --------------------------------------------\n","\n","# --------------------------------------------\n","# Valido número de ingreso por el usuario\n","if (Max_tiempo_sleep <= 0 or Min_tiempo_sleep <= 0 ):\n","  raise Exception(\"Por favor, ingrese números positivos.\") \n","# -------------\n","\n","# --------------------------------------------\n","# Constantes de comunicacion\n","WORK_FLAG = 1\n","END_WORK_FLAG = 2\n","# --------------------------------------------\n","\n","\n","def main():\n","    comm = MPI.COMM_WORLD # Instanciamos el tipo de comunicador a utilizar.\n","    id = comm.Get_rank() # Obtenemos el id como atributo del proceso que se ejecuta.\n","\n","    # Utilizamos el 0 para definir al procesos Maestro, cualquier otro id sera un esclavo.\n","    if (id == 0) :\n","        init() # Llamamos funcion init para eventos que requeriremos inicialmente solo 1 vez.\n","        numProcesses = comm.Get_size()  # Obtenemos el numero de procesos totales ejecutados.\n","        numTasks = (numProcesses-1)*4 # Se setea el numero de tareas.\n","        workTimes = generateTasks(numTasks) # Se generan las tareas, en este caso seran \n","        print(\"Master crea {} valores para el sleep de los slaves:\".format(workTimes.size), flush=True)\n","        print(workTimes, flush=True)\n","        initWork(comm, workTimes, numProcesses)\n","    else:\n","        doWork(comm)\n","\n","# Funcion que tendra lugar 1 sola vez al crearse el Master.\n","def init():\n","  print()\n","  print( \"------------------------------------\", flush=True)\n","  print( \"Informacion General:\", flush=True)\n","  print()\n","  # Imprimimos la version de la bibliotica MPI4py\n","  print (\"Version MPI4py utilizada: {}\".format(MPI.Get_version()), flush=True)\n","  print()\n","  # Imprimimos el procesador utilizado\n","  print (\"Procesador: {}\".format(MPI.Get_processor_name()), flush=True)\n","  print()\n","  print( \"------------------------------------\", flush=True)\n","  print()\n","\n","\n","def initWork(comm, workTimes, numProcesses):\n","    totalWork = workTimes.size\n","    workcount = 0\n","    recvcount = 0\n","    print(\"Master enviando las tareas:\", flush=True)\n","    for id in range(1, numProcesses):\n","        if workcount < totalWork:\n","            work=workTimes[workcount]\n","            comm.send(work, dest=id, tag=WORK_FLAG) # Envia mensaje de iniciar trabajo con el dato correspondiente del array.\n","            workcount += 1\n","            print(\"Master envia {} al slave: {}\".format(work, id), flush=True)\n","    print( \"------------------------------------\", flush=True)\n","\n","    # Mientras haya trabajo, se recibe el resultado de los slaves y se sigue enviando trabajo.\n","    while (workcount < totalWork) :\n","        stat = MPI.Status() \n","        workTime = comm.recv(source=MPI.ANY_SOURCE, status=stat) # Recivimos resultados de los slaves.\n","        recvcount += 1\n","        workerId = stat.Get_source() # Obtenemos el numero de slave.\n","        print(\"Master recibe {} del slave: {}\".format(workTime, workerId), flush=True)\n","        #send next work\n","        comm.send(workTimes[workcount], dest=workerId, tag=WORK_FLAG) # Funcion bloqueante, comparte un buffer de memoria entre el llamador y el llamado, el llamador no continua su ejecucion hasta que este buffer sea liberado. \n","        workcount += 1\n","        print(\"Master envia {} al slave: {}\".format(work, workerId), flush=True)\n","\n","    # Recivir los resultados de las peticiones de trabajo.\n","    while (recvcount < totalWork):\n","        stat = MPI.Status()\n","        workTime = comm.recv(source=MPI.ANY_SOURCE, status=stat)\n","        recvcount += 1\n","        workerId = stat.Get_source()\n","        print(\"Fin: Master recibe {} del slave: {}\".format(workTime, workerId), flush=True)\n","\n","    # Parar a todos los slaves.\n","    for id in range(1, numProcesses):\n","        comm.send(-1, dest=id, tag=END_WORK_FLAG)\n","\n","\n","def doWork(comm):\n","    # El slave realizara sus tareas hasta que reciba un mensaje de finalizar desde el Master.\n","    while(True):\n","        stat = MPI.Status() # Obtiene el estado actual del slave.\n","        waitTime = comm.recv(source=0, tag=MPI.ANY_TAG, status=stat) # Obtiene lo enviado por el Master.\n","        print(\"Slave {} recibe {}\".format(comm.Get_rank(), waitTime), flush=True)\n","        # Si el Master envia que debe finalizar, entonces no se realiza ninguna tarea y retorna.\n","        if (stat.Get_tag() == END_WORK_FLAG):\n","            print(\"Slave: {} finaliza su trabajo.\".format(comm.Get_rank()), flush=True)\n","            return\n","        time.sleep(waitTime) # Realiza el trabajo\n","        comm.send(waitTime, dest=0) # Envia el fin del trabajo al Master\n","\n","def generateTasks(numTasks):\n","    np.random.seed(1000)  # Cambiar la semilla del random para que se generen efectivamente diferentes numeros.\n","    return np.random.randint(low=Min_tiempo_sleep, high=Max_tiempo_sleep, size=numTasks)\n","\n","# Ejecutar la funcion principal.\n","main()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"anJw5JH-443-"},"source":["## 3.1 Ejecutar prueba del desarollo"]},{"cell_type":"code","metadata":{"id":"NTEtBCIa4-LP"},"source":["! mpirun --allow-run-as-root -np 4 python Ejercicio3.py"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9cE38A1-IUPQ"},"source":["#4 Tabla de pasos de ejecución del programa\n","\n","\n"," Procesador | Función | Detalle\n","------------|---------|----------\n","CPU      |  @param                | Lectura del tiempo máximo que debe dormir un slave.\n","CPU      |  @param                | Lectura del tiempo mínimo que debe dormir un slave.\n","CPU      |  constantes            | Bloque de definición de constantes.\n","CPU      |  Main - MPI.COMM_WORLD        | Inicializa el comunicador entre procesos.\n","CPU      |  Main - comm.Get_rank()      | Obtiene el proceso actual que se ejecuta.\n","CPU      |  Main - init()             | Muestra información general.\n","CPU      |  Main - comm.Get_size()    | Obtiene la cantidad de procesos totales.\n","CPU      |  Main - generateTasks(numTasks)       | Genera las tareas en un array.\n","CPU      |  Main - initWork(comm, workTimes, numProcesses) | El Master inicia las peticiones de trabajo.\n","CPU      |  Main - doWork(comm)         | Los slaves tienen asignada la funcion para su trabajo.\n","CPU      |  initWork - comm.send(work, dest=id, tag=WORK_FLAG)            | El Master envía el trabajo al slave.\n","CPU      | doWork - comm.recv(source=0, tag=MPI.ANY_TAG, status=stat)  | El Slave recibe el trabajo del Master.\n","CPU      | doWork - comm.send(waitTime, dest=0)                        | El Slave le envia la respuesta de trabajo realizado al Master.\n","CPU      |  initWork - comm.recv(source=MPI.ANY_SOURCE, status=stat)   | El Master recibe el mensaje de trabajo realizado por el slave.\n","CPU      |  initWork - comm.send(-1, dest=id, tag=END_WORK_FLAG)               | El Master envía el mensaje para informar a los slaves del fin del trabajo.\n"]},{"cell_type":"markdown","metadata":{"id":"zkB_eGvlI7D2"},"source":["#5 Conclusiones\n","\n","La librería MPI4py provee una interfaz sencilla y práctica para la gestión de tareas concurrentes en distintos procesos sin la necesidad de codificar distintos hilos y realizar una sincronización entre ellos.\n","\n","La primer duda que tuve al empezar con la codificación del ejercicio fue en que momento se instancia cada hilo, esto sucede directamente al momento donde importamos la librería o, si es deseado, se pueden invocar funciones como **MPI.Init()** o **MPI.Init_thread()** para hacerlo de manera más manual.\n","\n","Esta librería provee una amplia variedad de funciones con variedad de usos y la consideré especialmente útil a la hora de generar un patrón Master-Slave debido a que para el Master es posible comunicarse con uno solo de los Slave o en modo broadcast con todos ellos de manera sincroniza o asincrónica.\n","\n","Debido al límite de tiempo no he podido explorar la totalidad de esta librería, no obstante, este ejercicio posee muchas formas de ser ampliado o mejorado. Por ejemplo, es posible implementar un modelo productor consumidor de manera sencilla o realizar algunas tareas de manera asincrónica y otras sincronizadas.\n"]},{"cell_type":"markdown","metadata":{"id":"ChP75EDxI89c"},"source":["#6 Referencias\n","\n","\n","\n","*   [1] https://mpi4py.readthedocs.io/en/stable/\n","\n"]}]}